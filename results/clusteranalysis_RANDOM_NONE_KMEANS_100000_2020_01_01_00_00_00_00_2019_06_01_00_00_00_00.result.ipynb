{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.033694,
     "end_time": "2020-04-22T10:12:16.442634",
     "exception": false,
     "start_time": "2020-04-22T10:12:16.408940",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "papermill": {
     "duration": 1.327623,
     "end_time": "2020-04-22T10:12:17.804406",
     "exception": false,
     "start_time": "2020-04-22T10:12:16.476783",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from google.cloud import bigquery\n",
    "import time\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import seaborn as sns\n",
    "import simplejson as json\n",
    "import os\n",
    "from datetime import timedelta\n",
    "import webbrowser\n",
    "import papermill as pm\n",
    "from enum import Enum\n",
    "import re\n",
    "import simplejson as json\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.cluster import OPTICS\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "papermill": {
     "duration": 0.044814,
     "end_time": "2020-04-22T10:12:17.882688",
     "exception": false,
     "start_time": "2020-04-22T10:12:17.837874",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: GOOGLE_APPLICATION_CREDENTIALS=./secrets/bigquery-service-account.json\n"
     ]
    }
   ],
   "source": [
    "# InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%env GOOGLE_APPLICATION_CREDENTIALS=./secrets/bigquery-service-account.json\n",
    "client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.033175,
     "end_time": "2020-04-22T10:12:17.949167",
     "exception": false,
     "start_time": "2020-04-22T10:12:17.915992",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "papermill": {
     "duration": 0.043078,
     "end_time": "2020-04-22T10:12:18.025979",
     "exception": false,
     "start_time": "2020-04-22T10:12:17.982901",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_number_of_clusters = 30\n",
    "min_number_of_clusters = 2\n",
    "\n",
    "class OUTLIER_REMOVAL(Enum):\n",
    "    NONE = 1 \n",
    "    VARIANCE_THRESHOLD_WITHIN_CLUSTER = 2 \n",
    "    ISOLATION_FOREST = 3 \n",
    "\n",
    "outlier_removal = OUTLIER_REMOVAL.VARIANCE_THRESHOLD_WITHIN_CLUSTER.value\n",
    "\n",
    "class CLUSTER_METHOD(Enum):\n",
    "    KMEANS = 1 \n",
    "    OPTICS = 2 \n",
    "\n",
    "cluster_method = CLUSTER_METHOD.OPTICS.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "papermill": {
     "duration": 0.042483,
     "end_time": "2020-04-22T10:12:18.102068",
     "exception": false,
     "start_time": "2020-04-22T10:12:18.059585",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "number_of_addresses = 1000\n",
    "\n",
    "# format: yyyy-mm-dd\n",
    "observation_period_start = \"2020-01-01 00:00:00+00\"\n",
    "observation_period_end = \"2020-02-01 00:00:00+00\"\n",
    "\n",
    "class ADDRESS_SELECTION(Enum):\n",
    "    RANDOM = 1 # selects random features.index, that have been active within the observation period.\n",
    "    RICHEST = 2 # selects the accounts that have the most ether # not yet implemented\n",
    "    HIGHEST_TURNOVER = 3 # selects the accounts that have the most ether received + sent\n",
    "\n",
    "address_selection = ADDRESS_SELECTION.RICHEST.value\n",
    "\n",
    "# max USD amount to spent for executing sql queries\n",
    "max_bigquery_costs_usd = 2\n",
    "\n",
    "# Delete existing tables\n",
    "reset = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "papermill": {
     "duration": 0.041877,
     "end_time": "2020-04-22T10:12:18.177447",
     "exception": false,
     "start_time": "2020-04-22T10:12:18.135570",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "number_of_addresses = 100000\n",
    "observation_period_start = \"2020-01-01 00:00:00+00\"\n",
    "observation_period_end = \"2019-06-01 00:00:00+00\"\n",
    "address_selection = 1\n",
    "max_bigquery_costs_usd = 2\n",
    "reset = False\n",
    "outlier_removal = 1\n",
    "cluster_method = 1\n",
    "max_number_of_clusters = 30\n",
    "min_number_of_clusters = 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.033525,
     "end_time": "2020-04-22T10:12:18.244421",
     "exception": false,
     "start_time": "2020-04-22T10:12:18.210896",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Create feature table in bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.751831,
     "end_time": "2020-04-22T10:12:19.030205",
     "exception": false,
     "start_time": "2020-04-22T10:12:18.278374",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    pm.execute_notebook(\n",
    "       './bigquery/features.ipynb',\n",
    "       './build/features.build.ipynb',\n",
    "       parameters = dict(number_of_addresses=number_of_addresses, observation_period_start=observation_period_start,observation_period_end=observation_period_end, address_selection=address_selection,max_bigquery_costs_usd=max_bigquery_costs_usd, reset = reset),\n",
    "       cwd = \"./bigquery\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    if \"does already exist\" in e.args[0]:\n",
    "        print(\"Feature table does already exist.\")\n",
    "        pass\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load feature table from bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "table_name = \"features\"\n",
    "table_id = \"{}_{}_{}_{}_{}\".format(table_name, ADDRESS_SELECTION(address_selection).name, number_of_addresses, re.sub(r'[-.+: ]', '_', observation_period_start),re.sub(r'[-.+: ]', '_', observation_period_end))\n",
    "\n",
    "filename = '{}.json'.format(table_id)\n",
    "data_dir = \"./data\"\n",
    "\n",
    "if filename not in os.listdir(\"./data\") or reset == True:\n",
    "     \n",
    "    print(\"Loading data from bigquery ...\")\n",
    "    \n",
    "    sql = \"\"\"\n",
    "        SELECT *\n",
    "        FROM `masterarbeit-245718.ethereum_us.{table_id_features}` \n",
    "        \"\"\".format(table_id_features = table_id)\n",
    "\n",
    "    query_job = client.query(sql)  \n",
    "    \n",
    "    start = time.time();\n",
    "    \n",
    "    features = query_job.result().to_dataframe(); \n",
    "    features = features.set_index(\"address\")\n",
    "    \n",
    "    done = time.time();\n",
    "    elapsed = round(done - start);\n",
    "    \n",
    "    print(\"Time to retrieve data from BigQuery: {} Minutes.\".format(round(elapsed/60)))\n",
    "    \n",
    "    with open('{}/{}'.format(data_dir, filename), 'w') as json_file:\n",
    "        json.dump(features.to_dict(), json_file, use_decimal=True, default=str)\n",
    "        \n",
    "else: \n",
    "    \n",
    "    print(\"Loading data from local cache ...\")\n",
    "    \n",
    "    start = time.time();\n",
    "    \n",
    "    with open('./data/{}'.format(filename), \"r\") as file:  \n",
    "        file_content_json = json.load(file);\n",
    "        \n",
    "    done = time.time();\n",
    "    elapsed = round(done - start);\n",
    "    \n",
    "    features = pd.DataFrame(file_content_json) \n",
    "    \n",
    "    print(\"Time to retrieve data from local cache: {} Seconds.\".format(elapsed))\n",
    "        \n",
    "features = features.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "m = features.memory_usage();\n",
    "total_bytes = m.sum();\n",
    "total_megabytes = total_bytes/10**6;\n",
    "print(\"Size of the account feature dataset: {} Megabytes.\".format(round(total_megabytes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Set avg_time_diff and stddev 0 values to max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "features[\"avg_time_diff_sent_tx\"] = features[\"avg_time_diff_sent_tx\"].replace(to_replace=0.0, value=max(features[\"avg_time_diff_sent_tx\"]))\n",
    "features[\"avg_time_diff_received_tx\"] = features[\"avg_time_diff_received_tx\"].replace(to_replace=0.0, value=max(features[\"avg_time_diff_received_tx\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "features[\"stddev_received_tx\"] = features[\"stddev_received_tx\"].replace(to_replace=0.0, value=max(features[\"stddev_received_tx\"]))\n",
    "features[\"stddev_sent_tx\"] = features[\"stddev_sent_tx\"].replace(to_replace=0.0, value=max(features[\"stddev_sent_tx\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# features = features.drop(columns=[\"stddev_received_tx\",\n",
    "#                                   \"stddev_sent_tx\",\n",
    "#                                   \"avg_time_diff_sent_tx\",\n",
    "#                                   \"avg_time_diff_received_tx\",\n",
    "#                                   \"balance\",\n",
    "#                                   \"usd_received\",\n",
    "#                                   \"usd_sent\",\n",
    "#                                   \"avg_usd_sent\",\n",
    "#                                   \"avg_usd_received\",\n",
    "#                                   \"monthly_usd_sent\",\n",
    "#                                   \"monthly_usd_received\",\n",
    "#                                   \"monthly_outgoing_txns\",\n",
    "#                                   \"monthly_incoming_txns\",\n",
    "#                                   \"avg_wei_sent\",\n",
    "#                                   \"avg_wei_received\",\n",
    "#                                   \"monthly_wei_sent\",\n",
    "#                                   \"monthly_wei_received\"                                 \n",
    "#                                  ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Standardise feature values, so that all feature values have the same mean and stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler(copy=True, with_mean=True, with_std=True);\n",
    "features_std = scaler.fit_transform(features);\n",
    "features_std = pd.DataFrame(features_std, columns=features.columns, index=features.index);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Use Isolationforest to identify outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf = IsolationForest(max_samples=\"auto\", behaviour='new', contamination='auto')\n",
    "\n",
    "clf.fit(features_std)\n",
    "pred = clf.predict(features_std)\n",
    "\n",
    "features_clean = features[pred == 1]\n",
    "features_std_clean = features_std[pred == 1]\n",
    "\n",
    "outliers = features[pred == -1]\n",
    "\n",
    "print(\"Number of detected outliers: {}\".format(outliers.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display outliers\n",
    "outliers_tmp = outliers.copy()\n",
    "outliers_tmp[\"turnover\"] = outliers[\"wei_received\"] + outliers[\"wei_sent\"]\n",
    "outliers_tmp = outliers_tmp.sort_values(by=[\"turnover\"], ascending=False)\n",
    "outliers_tmp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # opens outliers features.index via etherscan\n",
    "# n = 5 # first n features.index get opened\n",
    "# i = 0\n",
    "# for address, features in outliers_tmp.iterrows():\n",
    "#     webbrowser.open('https://etherscan.io/address/{}'.format(address), new=2)\n",
    "#     i += 1\n",
    "#     if i == n:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Use Isolationforest to remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if outlier_removal == OUTLIER_REMOVAL.ISOLATION_FOREST.value:\n",
    "    features = features_clean\n",
    "    features_std = features_std_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dimension Reduction via Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Frage:** Wie viel Varianz sollte von der Teilmenge der Principal Components, welche die meiste Varianz erklären, mindestens erklärt werden? Im Folgenden treffe ich die Annahme, dass 90% ausreicht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "min_explained_variance = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pca = PCA().fit(features_std)\n",
    "optimal_number_of_clusters = 0\n",
    "for i in np.cumsum(pca.explained_variance_ratio_):\n",
    "    optimal_number_of_clusters = optimal_number_of_clusters + 1\n",
    "    if i > min_explained_variance:\n",
    "        break;\n",
    "\n",
    "print(\"Die {} ersten PCA-Komponenten erklären mehr als {}% der Varianz.\".format(optimal_number_of_clusters, min_explained_variance*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(range(1, len(pca.explained_variance_ratio_) + 1, 1), np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Variance (%)') #for each component\n",
    "plt.title('PCA: Explained Variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=optimal_number_of_clusters);\n",
    "features_std_pca = pca.fit_transform(features_std);\n",
    "features_std_pca = pd.DataFrame(features_std_pca);\n",
    "features_std_pca.index = features.index\n",
    "features_std_pca.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Caluclate number of clusters via Clinski Harabasz Score (only when using K-MEANS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def estimateNumberOfClusters(features, features_std_pca, max_number_of_clusters):\n",
    "    scores = []\n",
    "    numberOfCluster = []\n",
    "\n",
    "    for i in range(2,max_number_of_clusters,1):\n",
    "        kmeans_tmp = KMeans(n_clusters=i)\n",
    "        kmeans_tmp.fit(features_std_pca)\n",
    "        numberOfCluster.append(i)\n",
    "        scores.append(metrics.calinski_harabasz_score(features, kmeans_tmp.labels_))  \n",
    "    \n",
    "    result = pd.DataFrame(np.transpose(np.array([numberOfCluster, scores])).astype(int), columns=[\"Number of Clusters\", \"C.H. Score\"]);\n",
    "    optimal_number_of_clusters = numberOfCluster[np.argmax(scores)];\n",
    "    \n",
    "    return result, optimal_number_of_clusters;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plotScores(scores):\n",
    "    plt.figure()\n",
    "    plt.bar(scores.iloc[:, 0],scores.iloc[:, 1] )\n",
    "    plt.xlabel('Number of Clusters')\n",
    "    plt.ylabel('C.H. Score') #for each component\n",
    "    plt.title('Calinski Harabasz Score')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if cluster_method == CLUSTER_METHOD.KMEANS.value:\n",
    "    scores = []\n",
    "    numberOfCluster = []\n",
    "    \n",
    "    # 3 clusters are not considered to be valid\n",
    "    for i in range(min_number_of_clusters, max_number_of_clusters, 1):\n",
    "        kmeans_tmp = KMeans(n_clusters=i)\n",
    "        kmeans_tmp.fit(features_std_pca)\n",
    "        numberOfCluster.append(i)\n",
    "        scores.append(metrics.calinski_harabasz_score(features, kmeans_tmp.labels_))  \n",
    "    \n",
    "    results = pd.DataFrame(zip(numberOfCluster, scores), columns=[\"number_of_clusters\", \"calinski_score\"])\n",
    "    optimal_number_of_clusters = results.loc[results[\"calinski_score\"].idxmax()].astype(int)[\"number_of_clusters\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "if cluster_method == CLUSTER_METHOD.KMEANS.value:\n",
    "    plt.figure(figsize=(10,5))\n",
    "    bars = plt.bar(x=results[\"number_of_clusters\"], height=results[\"calinski_score\"])\n",
    "    plt.xlabel(results.columns[0])\n",
    "    plt.ylabel(results.columns[1]) \n",
    "    bars[results[\"calinski_score\"].idxmax()].set_color(\"r\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    print(\"The optimal number of clusters is {}.\".format(optimal_number_of_clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  optimal_number_of_clusters = 5 # tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if cluster_method == CLUSTER_METHOD.KMEANS.value:\n",
    "    cluster_model = KMeans(n_clusters=optimal_number_of_clusters)\n",
    "\n",
    "if cluster_method == CLUSTER_METHOD.OPTICS.value:\n",
    "    cluster_model = OPTICS()\n",
    "\n",
    "cluster_model.fit(features_std_pca)\n",
    "features[\"label\"] = cluster_model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "filename = \"clustering_result_{}\".format(table_id)\n",
    "with open('{}/{}'.format(data_dir, filename), 'w') as json_file:\n",
    "    json.dump(features.to_dict(), json_file, use_decimal=True, default=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Remove outliers for each cluster via variance threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "features_std_pca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_outliers(features, cluster_model, threshold):\n",
    "    \n",
    "    features_without_outliers = pd.DataFrame()\n",
    "    labels_without_outliers = []\n",
    "\n",
    "    for label in np.unique(cluster_model.labels_):\n",
    "        centroid = np.array(features_std_pca[cluster_model.labels_ == label].mean())\n",
    "\n",
    "        features_within_cluster = features[cluster_model.labels_ == label];\n",
    "        # distance of each point in cluster to cluster's centroid\n",
    "        distances = features_within_cluster - centroid;\n",
    "        # convert to scalar\n",
    "        distances_norm = np.linalg.norm(distances, axis=1);\n",
    "        # max_distance for this cluster\n",
    "        max_distance = threshold * np.linalg.norm(features_within_cluster.std())\n",
    "        \n",
    "        keep = distances_norm <= max_distance\n",
    "        \n",
    "        features_without_outliers = features_without_outliers.append(features_within_cluster[keep])\n",
    "        labels_without_outliers.extend([label for i in features_within_cluster[keep].iterrows()])\n",
    "        \n",
    "    features_without_outliers[\"label\"] = labels_without_outliers\n",
    "    return features_without_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if outlier_removal == OUTLIER_REMOVAL.VARIANCE_THRESHOLD_WITHIN_CLUSTER.value:\n",
    "    from sklearn.metrics import calinski_harabaz_score as ch_score\n",
    "    \n",
    "    thresholds = np.linspace(.0001,.6, 20)\n",
    "    \n",
    "    features_tables_without_outliers = [remove_outliers(features_std_pca, cluster_model, t) for t in thresholds]\n",
    "    \n",
    "    scores= []\n",
    "    res = []\n",
    "    \n",
    "    for f,t in zip(features_tables_without_outliers, thresholds):\n",
    "        if not f.empty and len(f[\"label\"].unique()) > 1 and len(f) > 4:\n",
    "            scores.append((metrics.calinski_harabasz_score(f.drop(columns=[\"label\"]), f[\"label\"]), t))\n",
    "            res.append(f)\n",
    "            \n",
    "    scores = [x for x in zip(*scores)]\n",
    "    \n",
    "    plt.plot(scores[1], scores[0])\n",
    "    plt.title('Calinski Harabaz Scores as std threshold changes')    \n",
    "    \n",
    "    optimal_threshold = thresholds[np.argmax(scores)]\n",
    "    features_std_pca_without_outliers = res[np.argmax(scores)]\n",
    "    features = features.reindex(features_std_pca_without_outliers.index)\n",
    "    features_std = features_std.reindex(features_std_pca_without_outliers.index)\n",
    "    features_std_pca = features_std_pca.reindex(features_std_pca_without_outliers.index)  \n",
    "    \n",
    "    print(\"The threshold leading to the highest Calinski Harabaz Score is\", optimal_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Visualisierung: T-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sklearn.manifold import TSNE\n",
    "\n",
    "# time_start = time.time()\n",
    "\n",
    "# tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=500)\n",
    "# features_std_pca_out_tsne = tsne.fit_transform(features_std_pca_out)\n",
    "# features_std_pca_out_tsne = pd.DataFrame(features_std_pca_out_tsne, columns=[\"1_tsne_comp\",\"1_tsne_comp\"])\n",
    "# features_std_pca_out_tsne.head()\n",
    "\n",
    "# print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Plot Digits t-SNE\n",
    "# import seaborn as sns\n",
    "\n",
    "# # Set style of scatterplot\n",
    "# sns.set_context(\"notebook\", font_scale=1.1)\n",
    "# sns.set_style(\"ticks\")\n",
    "\n",
    "# features_std_pca_out_tsne[\"Label\"] = cluster_model.labels_\n",
    "# features_std_pca_out_tsne.columns = [\"x\", \"y\", \"Label\"]\n",
    "\n",
    "# sns.lmplot(x='x',\n",
    "#            y='y',\n",
    "#            data=features_std_pca_out_tsne,\n",
    "#            fit_reg=False,\n",
    "#            legend=True,\n",
    "#            height=9,\n",
    "#            hue='Label',\n",
    "#            scatter_kws={\"s\":200, \"alpha\":0.3})\n",
    "\n",
    "\n",
    "# plt.title('t-SNE Results:', weight='bold').set_fontsize('14')\n",
    "# plt.xlabel('Prin Comp 1', weight='bold').set_fontsize('10')\n",
    "# plt.ylabel('Prin Comp 2', weight='bold').set_fontsize('10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Display principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D;\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10));\n",
    "ax = Axes3D(fig, elev=-150, azim=110);\n",
    "\n",
    "pca_tmp = PCA(n_components=3)\n",
    "features_std_pca_tmp = pca_tmp.fit_transform(features_std);\n",
    "features_std_pca_tmp = pd.DataFrame(features_std_pca_tmp)\n",
    "features_std_pca_tmp.index = features.index \n",
    "features_std_pca_tmp.columns = [\"1st eigenvector\", \"2st eigenvector\", \"3st eigenvector\"] \n",
    "features_std_pca_tmp[\"label\"] = features[\"label\"]\n",
    "\n",
    "labels = np.sort(features_std_pca_tmp.label.unique())\n",
    "\n",
    "for l in labels:\n",
    "    ax.scatter(features_std_pca_tmp[\"1st eigenvector\"][features_std_pca_tmp[\"label\"] == l], \n",
    "           features_std_pca_tmp[\"2st eigenvector\"][features_std_pca_tmp[\"label\"] == l],\n",
    "           features_std_pca_tmp[\"3st eigenvector\"][features_std_pca_tmp[\"label\"] == l],\n",
    "           label = l, \n",
    "           edgecolor='k', s=40, alpha = 1);\n",
    "\n",
    "ax.set_title(\"First three PCA Components\");\n",
    "ax.set_xlabel(\"1st eigenvector\");\n",
    "ax.w_xaxis.set_ticklabels([]);\n",
    "ax.set_ylabel(\"2nd eigenvector\");\n",
    "ax.w_yaxis.set_ticklabels([]);\n",
    "ax.set_zlabel(\"3rd eigenvector\");\n",
    "ax.w_zaxis.set_ticklabels([]);\n",
    "ax.legend(title= \"Clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Display cluster means "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "clusters = dict() \n",
    "labels = features[\"label\"].unique()\n",
    "labels.sort()\n",
    "\n",
    "for label in labels:\n",
    "    cluster_name = 'cluster_{}'.format(label)\n",
    "    \n",
    "    features_in_cluster = features[features.label == label]\n",
    "    \n",
    "    clusters[cluster_name] = dict()\n",
    "    clusters[cluster_name][\"number_of_addresses\"] = Counter(features[\"label\"])[label]\n",
    "    clusters[cluster_name][\"feature_means\"] = features_in_cluster.drop(columns=[\"label\"]).mean()\n",
    "    \n",
    "feature_means = pd.DataFrame(columns = clusters[cluster_name]['feature_means'].keys())\n",
    "\n",
    "for key in clusters:\n",
    "    clusters[key][\"feature_means\"].name = key\n",
    "    feature_means = feature_means.append(clusters[key][\"feature_means\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_means_display = feature_means.round()\n",
    "\n",
    "if \"wei_received\" in feature_means_display.columns:\n",
    "    feature_means_display[\"eth_received\"] = feature_means_display[\"wei_received\"] / 10**18\n",
    "    feature_means_display = feature_means_display.drop(columns=[\"wei_received\"])\n",
    "\n",
    "if \"wei_sent\" in feature_means_display.columns:\n",
    "    feature_means_display[\"eth_sent\"] = feature_means_display[\"wei_sent\"] / 10**18\n",
    "    feature_means_display = feature_means_display.drop(columns=[\"wei_sent\"])\n",
    "\n",
    "    \n",
    "if \"balance\" in feature_means_display.columns:\n",
    "    feature_means_display[\"eth_balance\"] = feature_means_display[\"balance\"] / 10**18\n",
    "    feature_means_display = feature_means_display.drop(columns=[\"balance\"])\n",
    "    \n",
    "if \"avg_wei_sent\" in feature_means_display.columns:\n",
    "    feature_means_display[\"avg_eth_sent\"] = feature_means_display[\"avg_wei_sent\"] / 10**18\n",
    "    feature_means_display = feature_means_display.drop(columns=[\"avg_wei_sent\"])\n",
    "    \n",
    "if \"avg_wei_received\" in feature_means_display.columns:\n",
    "    feature_means_display[\"avg_eth_received\"] = feature_means_display[\"avg_wei_received\"] / 10**18\n",
    "    feature_means_display = feature_means_display.drop(columns=[\"avg_wei_received\"])\n",
    "    \n",
    "if \"monthly_wei_sent\" in feature_means_display.columns:\n",
    "    feature_means_display[\"monthly_eth_sent\"] = feature_means_display[\"monthly_wei_sent\"] / 10**18\n",
    "    feature_means_display = feature_means_display.drop(columns=[\"monthly_wei_sent\"])\n",
    "    \n",
    "if \"monthly_wei_received\" in feature_means_display.columns:\n",
    "    feature_means_display[\"monthly_eth_received\"] = feature_means_display[\"monthly_wei_received\"] / 10**18\n",
    "    feature_means_display = feature_means_display.drop(columns=[\"monthly_wei_received\"])\n",
    "    \n",
    "feature_means_display[\"number_of_addresses\"] = [clusters[c][\"number_of_addresses\"] for c in clusters]\n",
    "\n",
    "if set([\"number_of_addresses\", \"eth_balance\", \"eth_sent\", \"eth_received\"]).issubset(set(feature_means_display.columns)):\n",
    "    cols = list(feature_means_display.columns.values) \n",
    "    cols.pop(cols.index('number_of_addresses')) \n",
    "    cols.pop(cols.index('eth_balance')) \n",
    "    cols.pop(cols.index('eth_sent')) \n",
    "    cols.pop(cols.index('eth_received')) \n",
    "    feature_means_display = feature_means_display[[\"number_of_addresses\", \"eth_balance\", \"eth_sent\", \"eth_received\"]+cols] \n",
    "\n",
    "\n",
    "if len(feature_means_display.columns) > len(feature_means_display.index):\n",
    "    print(\"Number of Addresses: {}\".format(number_of_addresses))\n",
    "    print(\"Observation-Period: {} to {}\".format(observation_period_start, observation_period_end))\n",
    "    display(feature_means_display.T.astype(\"int\"))\n",
    "else:\n",
    "    display(feature_means_display.astype(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# features[features[\"label\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize= (20,10))\n",
    "\n",
    "# scaler = StandardScaler(copy=True, with_mean=True, with_std=True);\n",
    "# features_std = scaler.fit_transform(features);\n",
    "# features_std = pd.DataFrame(features_std, columns=features.columns);\n",
    "# X_reduced = PCA(n_components=2).fit_transform(features_std);\n",
    "\n",
    "# ax.scatter(X_reduced[:, 0], X_reduced[:, 1], c=cluster_model.labels_)\n",
    "# ax.set_xlabel(\"Principal Component 1\")\n",
    "# ax.set_ylabel(\"Principal Component 2\")\n",
    "# pd.Series(cluster_model.labels_).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# features_std_pca_tmp.idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# snowflakes = features_std_pca_tmp.idxmax()\n",
    "# snowflakes\n",
    "# for i in snowflakes:\n",
    "#     print(features.index[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Display Cluster means with standardised features via radar plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cluster_data_for_plot1 = cluster_data.drop(columns=[\"cluster_size\"])\n",
    "# cluster_data_for_plot1 = cluster_data_for_plot1 / cluster_data_for_plot1.sum()\n",
    "# cluster_data_for_plot1[\"cluster_size\"] = cluster_data[\"cluster_size\"]\n",
    "# # cluster_data_for_plot1 = cluster_data_for_plot1.reset_index()\n",
    "\n",
    "# ax = cluster_data_for_plot1.plot.barh(x=\"cluster_size\" ,figsize=(15,15))\n",
    "\n",
    "# ax.tick_params(\n",
    "#     which=\"major\",\n",
    "#     axis='x', \n",
    "#     bottom=False\n",
    "#     )\n",
    "\n",
    "# ax.tick_params(\n",
    "#     which=\"major\",\n",
    "#     axis='y', \n",
    "#     labelsize=15\n",
    "#     ) \n",
    "\n",
    "# ylabels = ax.get_yticklabels()\n",
    "\n",
    "# ax.legend(loc=\"lower right\", fontsize=\"larger\")\n",
    "\n",
    "# ax.set_xticklabels([])\n",
    "# ylabel = ax.set_ylabel(\"Size of\\nCluster\", fontsize=\"xx-large\", rotation=0, labelpad=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "N = len(feature_means.columns)\n",
    "\n",
    "values = {}\n",
    "feature_std_means = pd.DataFrame(columns=features.columns[:-1])\n",
    "\n",
    "for label in labels:\n",
    "    features_in_cluster = features_std[features[\"label\"] == label]\n",
    "    feature_means_tmp = features_in_cluster.mean()\n",
    "    feature_means_tmp.name = 'cluster_{}'.format(label)\n",
    "    feature_std_means = feature_std_means.append(feature_means_tmp)\n",
    "\n",
    "# make all values positive, otherwise bigger values are displayed as if they were smaller \n",
    "feature_std_means_positive = feature_std_means + feature_std_means.max().max()\n",
    "\n",
    "for i,v in feature_std_means.iterrows(): \n",
    "    tmp_values = feature_std_means_positive.loc[i].values.tolist()\n",
    "    tmp_values += tmp_values[:1]\n",
    "    values[i] = tmp_values\n",
    "    \n",
    "angles = [n / float(N) * 2 * math.pi for n in range(N)]\n",
    "angles += angles[:1]\n",
    "\n",
    "fig = plt.figure(figsize=(25,len(values) * 10))\n",
    "\n",
    "i = 0\n",
    "\n",
    "for cluster_name, cluster_values in values.items():\n",
    "\n",
    "    i += 1\n",
    "    axis = plt.subplot(math.ceil(len(values)/2), 2, i, polar=True)\n",
    "    plt.sca(axis)\n",
    "    \n",
    "    plt.xticks(angles[:-1], features.columns, size=15)\n",
    "    \n",
    "    axis.plot(angles, cluster_values, linewidth=1, linestyle='solid')\n",
    "    axis.fill(angles, cluster_values, 'b', alpha=0.1)\n",
    "    axis.set_title(\"({}, addresses: {})\".format(cluster_name, clusters[cluster_name][\"number_of_addresses\"]), color = \"black\", size=25, loc=\"center\", pad=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from math import pi\n",
    "\n",
    "# cluster_data_for_plot2 = cluster_data_for_plot1.copy()\n",
    "# cluster_data_for_plot2 = cluster_data_for_plot2.drop(columns=[\"cluster_size\"])\n",
    "# cluster_data_for_plot2 = cluster_data_for_plot2.sort_index()\n",
    "# # cluster_data_for_plot2 = cluster_data_for_plot2.set_index(\"index\")\n",
    "\n",
    "# categories=list(cluster_data_for_plot2)\n",
    "# N = len(categories)\n",
    "\n",
    "# values = {}\n",
    "\n",
    "# for i,v in cluster_data_for_plot2.iterrows(): \n",
    "#     tmp_values = cluster_data_for_plot2.loc[i].values.tolist()\n",
    "#     tmp_values += tmp_values[:1]\n",
    "#     values[i] = tmp_values\n",
    "\n",
    "# angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "# angles += angles[:1]\n",
    "# angles\n",
    " \n",
    "# plt.figure(figsize=(30,40))\n",
    "\n",
    "# for cluster_label, cluster_values in values.items():\n",
    "#     position = int(\"\".join([str(33), str(cluster_label + 1)]))\n",
    "#     axis = plt.subplot(position, polar=True)\n",
    "#     plt.sca(axis)\n",
    "#     plt.xticks(angles[:-1], categories, size=15)\n",
    "\n",
    "# #     plt.ylim(top=1)\n",
    "\n",
    "#     axis.plot(angles, cluster_values, linewidth=1, linestyle='solid')\n",
    "#     axis.fill(angles, cluster_values, 'b', alpha=0.1)\n",
    "# #     a.text(0.1,0.1, cluster_size[i], color = \"red\", size=20)\n",
    "#     axis.set_title(\"Cluster with ID={} has {} addresses\".format(cluster_label, cluster_sizes[cluster_label]), color = \"black\", size=25, loc=\"center\", pad=30)\n",
    "#     axis.set_yticks([t for i,t in enumerate(axis.get_yticks()) if i%2!=0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# yticks = axis.get_yticks()\n",
    "# print(yticks)\n",
    "# newyticks = [t for i,t in enumerate(yticks) if i%2==0]\n",
    "# newyticks\n",
    "# # axis.set_yticks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for i,cd in cluster_data.iterrows():a\n",
    "#     x = features.index[cluster_model.labels_==i]\n",
    "#     print(\"Length of cluster with id={}: {}\".format(i, len(x)))\n",
    "#     display(x)\n",
    "#     print(\"############################\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Clustering Evaluation: Recall, Precision and F1 score "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Get addresses with labels from etherscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "addresses_with_es_labels = dict()\n",
    "\n",
    "files = os.listdir(\"./data\")\n",
    "relevant_files = [f for f in files if \"labelcloud\" in f]\n",
    "\n",
    "for f in relevant_files:\n",
    "    with open(\"./data/{}\".format(f), newline=\"\") as csvFile:\n",
    "        reader = csv.reader(csvFile, delimiter=\",\")       \n",
    "        addresses_with_es_labels[f.split(\"_\",1)[0]]  = [row[0] for row in reader if len(row) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "es_label = 'exchanges'\n",
    "etherscan_labels = pd.DataFrame(columns=[es_label])\n",
    "\n",
    "\n",
    "for addr in list(features.index):\n",
    "    if addr.lower() in addresses_with_es_labels[es_label]:\n",
    "        etherscan_labels = etherscan_labels.append(pd.Series(1, name=addr))\n",
    "   \n",
    "etherscan_labels = etherscan_labels.reindex(index=features.index)\n",
    "etherscan_labels = etherscan_labels.fillna(0)\n",
    "etherscan_labels[es_label] = etherscan_labels[es_label].astype(\"int\")\n",
    "\n",
    "features[\"es_label_{}\".format(es_label)] = etherscan_labels[es_label]\n",
    "features.head()  \n",
    "\n",
    "cluster_list = []\n",
    "\n",
    "for label in features.label.unique():\n",
    "    cluster_list.append(features[features[\"label\"] == label][\"es_label_{}\".format(es_label)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import collections as collections\n",
    "from math import factorial as fac\n",
    "\n",
    "def b(x, y):\n",
    "    try:\n",
    "        binom = fac(x) // fac(y) // fac(x - y)\n",
    "    except ValueError:\n",
    "        binom = 0\n",
    "    return binom\n",
    "\n",
    "Counter = collections.Counter\n",
    "\n",
    "num_doc= 0\n",
    "TP = 0\n",
    "FP = 0\n",
    "FN = 0\n",
    "TN = 0\n",
    "c_list = []\n",
    "\n",
    "# Calculate True Positives\n",
    "\n",
    "for c in range(0, len(cluster_list)): \n",
    "    # calculating num_doc count...\n",
    "    num_doc += len(cluster_list[c])\n",
    "\n",
    "    c = Counter(cluster_list[c])\n",
    "    c_list.append(c)\n",
    "    TP += b(c[1], 2) \n",
    "\n",
    "# Calculate False Positives\n",
    "    \n",
    "for c in range(0, len(cluster_list)): \n",
    "    c = Counter(cluster_list[c])\n",
    "    FP += c[1] * c[0]\n",
    "\n",
    "# Add all the cluster together\n",
    "\n",
    "sum = Counter()\n",
    "for c in c_list:\n",
    "    sum += c\n",
    "\n",
    "# Calculating False Negatives\n",
    "\n",
    "for ct in c_list:\n",
    "    fn_temp = 0 \n",
    "\n",
    "    fn_temp += ct[1]*(sum[1]-ct[1])\n",
    "    sum -= ct\n",
    "    FN += fn_temp\n",
    "\n",
    "print(\"TP is %d \" % TP)\n",
    "print(\"FP is %d \" % FP)\n",
    "print(\"FN is %d \" % FN)\n",
    "\n",
    "if TP > 0:\n",
    "    Precision = TP/(TP+FP)\n",
    "    Recall = TP/(TP+FN)\n",
    "    F1=(2*Recall*Precision)/(Recall+Precision)\n",
    "else:\n",
    "    Precision = 0\n",
    "    Recall = 0\n",
    "    F1 = 0\n",
    "    \n",
    "print(\"Precision is %.4f \" % Precision)\n",
    "print(\"Recall is %.4f \" % Recall)\n",
    "print(\"F1 is %.4f \" % F1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Helper functions to analyse data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# i = 0\n",
    "# for index, row in features[features.label == 3].iterrows():\n",
    "#     i += 1\n",
    "#     a = row.name\n",
    "#     webbrowser.open('https://etherscan.io/address/{}'.format(a), new=2)\n",
    "#     if i == 5:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def open_etherscan(cluster_id, number_of_addresses):\n",
    "#     if len(features.index[features.label==cluster_id]) > number_of_addresses:\n",
    "#         for a in np.random.choice(features.index[cluster_model.labels_==cluster_id], number_of_addresses):\n",
    "#             webbrowser.open('https://etherscan.io/address/{}'.format(a), new=2)\n",
    "#     else:\n",
    "#         for a in features.index[features.label==cluster_id]:\n",
    "#             webbrowser.open('https://etherscan.io/address/{}'.format(a), new=2)\n",
    "    \n",
    "# open_etherscan(1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import collections as collections\n",
    "\n",
    "# Counter = collections.Counter\n",
    "\n",
    "# cluster_list = [['a','a','a','a','a','b'],['a','b','b','b','b','c'],['a','a','c','c','c']]\n",
    "# num_doc= 0\n",
    "# positives = 0\n",
    "# negatives = 0\n",
    "# TP = 0\n",
    "# FP = 0\n",
    "# FN = 0\n",
    "# TN = 0\n",
    "# c_list = []\n",
    "# for c in range(0, len(cluster_list)): \n",
    "#     # calculating num_doc count...\n",
    "#     num_doc += len(cluster_list[c])\n",
    "\n",
    "#     # calculating positives...\n",
    "#     positives +=  (len(cluster_list[c])*(len(cluster_list[c])-1))/2\n",
    "\n",
    "#     # calculating TP...\n",
    "#     c = Counter(cluster_list[c])\n",
    "#     c_list.append(c)\n",
    "#     tp_temp = 0 \n",
    "#     for k,v in dict(c).items():\n",
    "#         if v>1:\n",
    "#             tp_temp += (v*(v-1))/2\n",
    "#     TP += tp_temp \n",
    "\n",
    "# FP = positives - TP\n",
    "# negatives = ((num_doc*(num_doc-1))/2) - positives\n",
    "# # Add all the cluster together\n",
    "# sum = Counter()\n",
    "\n",
    "# for c in c_list:\n",
    "#     sum += c\n",
    "\n",
    "# # calculating FN...\n",
    "# for ct in c_list:\n",
    "#     fn_temp = 0 \n",
    "#     for k,v in dict(ct).items():\n",
    "#         fn_temp += v*(sum[k]-v)\n",
    "#     sum -= ct\n",
    "#     FN += fn_temp\n",
    "# TN = negatives -FN\n",
    "# print(\"num_doc is %d \" % num_doc)\n",
    "# print(\"positives is %d \" % positives)\n",
    "# print(\"TP is %d \" % TP)\n",
    "# print(\"FP is %d \" % FP)\n",
    "# print(\"FN is %d \" % FN)\n",
    "# print(\"TN is %d \" % TN)\n",
    "\n",
    "# Precision = TP/(TP+FP)\n",
    "# print(\"Precision is %.2f \" % Precision)\n",
    "\n",
    "# Recall = TP/(TP+FN)\n",
    "# print(\"Recall is %.2f \" % Recall)\n",
    "\n",
    "# F1=(2*Recall*Precision)/(Recall+Precision)\n",
    "# print(\"F1 is %.2f \" % F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from urllib.request aimport urlopen, Request\n",
    "# from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# headers = {\"User-Aagent\": \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.3\",\n",
    "#           \"Cookie\": \"_ga=GA1.2.501875784.1580321678; etherscan_cookieconsent=True; __cfduid=d361eede6a818d85e197685c3b1fca6911585732139; ASP.NET_SessionId=ac5xtlkyswxx0jxagox3bc2d; _gid=GA1.2.644739241.1586167885\"}\n",
    "# reg_url = \"https://etherscan.io/accounts/label/exchange/1?ps=100\"\n",
    "# req = Request(url=reg_url, headers=headers) \n",
    "# html = urlopen(req).read().decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# res1 = re.findall(\"address/0x.{40}\", html)\n",
    "# res1 = re.findall(\"0x.{40}\", str(res1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from bs4 import BeautifulSoup\n",
    "# soup = BeautifulSoup(html, 'html.parser')\n",
    "# elements = soup.find_all(\"tr\")\n",
    "# elements = list(elements)[1:-1]\n",
    "# exchangeNames = [e.contents[1].get_text() for e in elements]\n",
    "# exchangeAddresses = [e.contents[0].get_text().strip(\" \") for e in elements]\n",
    "# display(len(exchangeNames))\n",
    "# display(len(exchangeAddresses))\n",
    "# # for i in e:\n",
    "# #     print(i.contents[1].get_text())\n",
    "# #     print(i.children[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# exchanges = [(a, \"e\") for i, a in enumerate(features.index) if a.startswith(\"exchange\")]\n",
    "# speculators = [(a, \"x\") for i, a in enumerate(features.index) if a.startswith(\"speculator\")]\n",
    "\n",
    "# exchanges = pd.DataFrame(zip(*exchanges)).T\n",
    "# speculators = pd.DataFrame(zip(*speculators)).T\n",
    "\n",
    "# exchanges.columns = [\"address\", \"should_actual_be_in_cluster\"]\n",
    "# speculators.columns = [\"address\", \"should_actual_be_in_cluster\"]\n",
    "\n",
    "# exchanges = exchanges.set_index(\"address\")\n",
    "# speculators = speculators.set_index(\"address\")\n",
    "\n",
    "# # features.index[cluster_model.labels_ == 2]\n",
    "\n",
    "# tmp = pd.DataFrame(zip(features.index, cluster_model.labels_), columns=[\"address\", \"is_in_cluster\"])\n",
    "# tmp = tmp.set_index(\"address\")\n",
    "\n",
    "# tmp1 = exchanges.append(speculators)\n",
    "\n",
    "# tmp1 = tmp1.join(tmp, on=\"address\", how=\"inner\")\n",
    "\n",
    "# cluster_list = []\n",
    "# for i in tmp1[\"is_in_cluster\"].unique():\n",
    "#     cluster_list.append(list(tmp1[tmp1[\"is_in_cluster\"] == i][\"should_actual_be_in_cluster\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import collections as collections\n",
    "# from math import factorial as fac\n",
    "\n",
    "# def b(x, y):\n",
    "#     try:\n",
    "#         binom = fac(x) // fac(y) // fac(x - y)\n",
    "#     except ValueError:\n",
    "#         binom = 0\n",
    "#     return binom\n",
    "\n",
    "# Counter = collections.Counter\n",
    "\n",
    "# num_doc= 0\n",
    "# TP = 0\n",
    "# FP = 0\n",
    "# FN = 0\n",
    "# TN = 0\n",
    "# c_list = []\n",
    "# for c in range(0, len(cluster_list)): \n",
    "#     # calculating num_doc count...\n",
    "#     num_doc += len(cluster_list[c])\n",
    "\n",
    "#     c = Counter(cluster_list[c])\n",
    "#     c_list.append(c)\n",
    "#     TP += b(c[\"e\"], 2) \n",
    "    \n",
    "# # Calculate False Positives\n",
    "    \n",
    "# for c in range(0, len(cluster_list)): \n",
    "#     c = Counter(cluster_list[c])\n",
    "#     FP += c[\"e\"] * c[\"x\"]\n",
    "\n",
    "# # Add all the cluster together\n",
    "# sum = Counter()\n",
    "# for c in c_list:\n",
    "#     sum += c\n",
    "\n",
    "# # calculating FN...\n",
    "# for ct in c_list:\n",
    "#     fn_temp = 0 \n",
    "\n",
    "#     fn_temp += ct[\"e\"]*(sum[\"e\"]-ct[\"e\"])\n",
    "#     sum -= ct\n",
    "#     FN += fn_temp\n",
    "\n",
    "\n",
    "# print(\"num_doc is %d \" % num_doc)\n",
    "# print(\"TP is %d \" % TP)\n",
    "# print(\"FP is %d \" % FP)\n",
    "# print(\"FN is %d \" % FN)\n",
    "# print(\"TN is %d \" % TN)\n",
    "\n",
    "# Precision = TP/(TP+FP)\n",
    "# print(\"Precision is %.2f \" % Precision)\n",
    "\n",
    "# Recall = TP/(TP+FN)\n",
    "# print(\"Recall is %.2f \" % Recall)\n",
    "\n",
    "# F1=(2*Recall*Precision)/(Recall+Precision)\n",
    "# print(\"F1 is %.2f \" % F1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize= (20,10))\n",
    "\n",
    "# cluster_list.sort(key=len, reverse = True)\n",
    "# ind = range(0, len(cluster_list))\n",
    "\n",
    "# numberOfExchanges = [Counter(cl)[\"e\"] for cl in cluster_list]\n",
    "# numberOfSpeculators = [Counter(cl)[\"x\"] for cl in cluster_list]\n",
    "\n",
    "# line1 = ax.barh(ind, numberOfExchanges)\n",
    "# line2 = ax.barh(ind, numberOfSpeculators, left=numberOfExchanges)\n",
    "\n",
    "# yticklabels = ax.get_yticklabels()\n",
    "\n",
    "# start, end = ax.get_ylim()\n",
    "# ax.set_ylim(-1,7)\n",
    "\n",
    "# ax.yaxis.set_ticks(np.arange(0, 7, 1))\n",
    "\n",
    "# ax.legend((line1, line2), ('Exchanges', 'Unkown'), loc=\"upper right\", fontsize=\"larger\")\n",
    "\n",
    "# yticklabels = ax.set_yticklabels(ind)\n",
    "\n",
    "# ax.tick_params(\n",
    "#     which=\"major\",\n",
    "#     axis='both', \n",
    "#     labelsize=13\n",
    "#     ) \n",
    "# ylabel = ax.set_ylabel(\"Cluster-ID\", fontsize=\"xx-large\", rotation=0, labelpad=60)\n",
    "# ylabel = ax.set_xlabel(\"Number of features.index\", fontsize=\"xx-large\", rotation=0, labelpad=30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Temporary stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import collections as collections\n",
    "\n",
    "# Counter = collections.Counter\n",
    "\n",
    "# cluster_list = [['a','a','a','a','a','b'],['a','b','b','b','b','c'],['a','a','c','c','c']]\n",
    "# num_doc= 0\n",
    "# positives = 0\n",
    "# negatives = 0\n",
    "# TP = 0\n",
    "# FP = 0\n",
    "# FN = 0\n",
    "# TN = 0\n",
    "# c_list = []\n",
    "# for c in range(0, len(cluster_list)): \n",
    "#     # calculating num_doc count...\n",
    "#     num_doc += len(cluster_list[c])\n",
    "\n",
    "#     # calculating positives...\n",
    "#     positives +=  (len(cluster_list[c])*(len(cluster_list[c])-1))/2\n",
    "\n",
    "#     # calculating TP...\n",
    "#     c = Counter(cluster_list[c])\n",
    "#     c_list.append(c)\n",
    "#     tp_temp = 0 \n",
    "#     for k,v in dict(c).items():\n",
    "#         if v>1:\n",
    "#             tp_temp += (v*(v-1))/2\n",
    "#     TP += tp_temp \n",
    "\n",
    "# FP = positives - TP\n",
    "# negatives = ((num_doc*(num_doc-1))/2) - positives\n",
    "# # Add all the cluster together\n",
    "# sum = Counter()\n",
    "\n",
    "# for c in c_list:\n",
    "#     sum += c\n",
    "\n",
    "# # calculating FN...\n",
    "# for ct in c_list:\n",
    "#     fn_temp = 0 \n",
    "#     for k,v in dict(ct).items():\n",
    "#         fn_temp += v*(sum[k]-v)\n",
    "#     sum -= ct\n",
    "#     FN += fn_temp\n",
    "# TN = negatives -FN\n",
    "# print(\"num_doc is %d \" % num_doc)\n",
    "# print(\"positives is %d \" % positives)\n",
    "# print(\"TP is %d \" % TP)\n",
    "# print(\"FP is %d \" % FP)\n",
    "# print(\"FN is %d \" % FN)\n",
    "# print(\"TN is %d \" % TN)\n",
    "\n",
    "# Precision = TP/(TP+FP)\n",
    "# print(\"Precision is %.2f \" % Precision)\n",
    "\n",
    "# Recall = TP/(TP+FN)\n",
    "# print(\"Recall is %.2f \" % Recall)\n",
    "\n",
    "# F1=(2*Recall*Precision)/(Recall+Precision)\n",
    "# print(\"F1 is %.2f \" % F1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Problem: Mit dem obigen Algorithmus wird die Güte des Clustering Algorithmus ingesamt bestimmt. Ich will aber nur die Güte des Clustering Modell anhand der Exchange Adressen bestimme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Lösung?: (#Paare im selben cluster) / (#Paare in unterschiedlichen Clustern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import collections as collections\n",
    "\n",
    "# Counter = collections.Counter\n",
    "\n",
    "# cluster_list = [['a','a','a','a','a','b'],['a','b','b','b','b','c'],['a','a','c','c','c']]\n",
    "# num_doc= 0\n",
    "# positives = 0\n",
    "# negatives = 0\n",
    "# TP = 0\n",
    "# FP = 0\n",
    "# FN = 0\n",
    "# TN = 0\n",
    "# c_list = []\n",
    "# for c in range(0, len(cluster_list)): \n",
    "#     # calculating num_doc count...\n",
    "#     num_doc += len(cluster_list[c])\n",
    "\n",
    "#     # calculating positives...\n",
    "#     positives +=  (len(cluster_list[c])*(len(cluster_list[c])-1))/2\n",
    "\n",
    "#     # calculating TP...\n",
    "#     c = Counter(cluster_list[c])\n",
    "#     c_list.append(c)\n",
    "#     tp_temp = 0 \n",
    "#     for k,v in dict(c).items():\n",
    "#         if v>1:\n",
    "#             tp_temp += (v*(v-1))/2\n",
    "#     TP += tp_temp \n",
    "\n",
    "# FP = positives - TP\n",
    "# negatives = ((num_doc*(num_doc-1))/2) - positives\n",
    "# # Add all the cluster together\n",
    "# sum = Counter()\n",
    "\n",
    "# for c in c_list:\n",
    "#     sum += c\n",
    "\n",
    "# # calculating FN...\n",
    "# for ct in c_list:\n",
    "#     fn_temp = 0 \n",
    "#     for k,v in dict(ct).items():\n",
    "#         fn_temp += v*(sum[k]-v)\n",
    "#     sum -= ct\n",
    "#     FN += fn_temp\n",
    "# TN = negatives -FN\n",
    "# print(\"num_doc is %d \" % num_doc)\n",
    "# print(\"positives is %d \" % positives)\n",
    "# print(\"TP is %d \" % TP)\n",
    "# print(\"FP is %d \" % FP)\n",
    "# print(\"FN is %d \" % FN)\n",
    "# print(\"TN is %d \" % TN)\n",
    "\n",
    "# Precision = TP/(TP+FP)\n",
    "# print(\"Precision is %.2f \" % Precision)\n",
    "\n",
    "# Recall = TP/(TP+FN)\n",
    "# print(\"Recall is %.2f \" % Recall)\n",
    "\n",
    "# F1=(2*Recall*Precision)/(Recall+Precision)\n",
    "# print(\"F1 is %.2f \" % F1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "papermill": {
   "duration": 3.362428,
   "end_time": "2020-04-22T10:12:19.065705",
   "environment_variables": {},
   "exception": null,
   "input_path": "./clusteranalysis.ipynb",
   "output_path": "./results/clusteranalysis_RANDOM_NONE_KMEANS_100000_2020_01_01_00_00_00_00_2019_06_01_00_00_00_00.result.ipynb",
   "parameters": {
    "address_selection": 1,
    "cluster_method": 1,
    "max_bigquery_costs_usd": 2,
    "max_number_of_clusters": 30,
    "min_number_of_clusters": 2,
    "number_of_addresses": 100000,
    "observation_period_end": "2019-06-01 00:00:00+00",
    "observation_period_start": "2020-01-01 00:00:00+00",
    "outlier_removal": 1,
    "reset": false
   },
   "start_time": "2020-04-22T10:12:15.703277",
   "version": "2.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}